{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from math import sqrt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "import collections\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import cast,float32\n",
    "from keras import backend as kb\n",
    "from statistics import mean, stdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_kmers(x, k):\n",
    "    kmer_arr = []\n",
    "    for seq in x:\n",
    "        \n",
    "        kmers = []\n",
    "        n_kmers = len(seq) - k + 1\n",
    "\n",
    "        for i in range(0,n_kmers):\n",
    "            kmer = seq[i:i + k]\n",
    "            kmers.append(kmer)\n",
    "        kmer_arr.append(kmers)\n",
    "     \n",
    "    return kmer_arr\n",
    "\n",
    "class Vocab:  #@save\n",
    "\n",
    "    def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):\n",
    "        if tokens is None:\n",
    "            tokens = []\n",
    "        if reserved_tokens is None:\n",
    "            reserved_tokens = []\n",
    "\n",
    "        counter = count_corpus(tokens)\n",
    "        self._token_freqs = sorted(counter.items(), key=lambda x: x[1],\n",
    "                                   reverse=True)\n",
    "\n",
    "        self.idx_to_token = ['<unk>'] + reserved_tokens\n",
    "        self.token_to_idx = {token: idx\n",
    "                             for idx, token in enumerate(self.idx_to_token)}\n",
    "        # self.idx_to_token, self.token_to_idx = [], dict()\n",
    "        for token, freq in self._token_freqs:\n",
    "            if freq < min_freq:\n",
    "                break\n",
    "            if token not in self.token_to_idx:\n",
    "                self.idx_to_token.append(token)\n",
    "                self.token_to_idx[token] = len(self.idx_to_token) - 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)\n",
    "\n",
    "    def __getitem__(self, tokens):\n",
    "        if not isinstance(tokens, (list, tuple)):\n",
    "            return self.token_to_idx.get(tokens, self.unk)\n",
    "        return [self.__getitem__(token) for token in tokens]\n",
    "\n",
    "    def to_tokens(self, indices):\n",
    "        if not isinstance(indices, (list, tuple)):\n",
    "            return self.idx_to_token[indices]\n",
    "        return [self.idx_to_token[index] for index in indices]\n",
    "\n",
    "    @property\n",
    "    def unk(self):  \n",
    "        return 0\n",
    "\n",
    "    @property\n",
    "    def token_freqs(self):\n",
    "        return self._token_freqs\n",
    "\n",
    "def count_corpus(tokens):  #@save\n",
    "\n",
    "\n",
    "    if len(tokens) == 0 or isinstance(tokens[0], list):\n",
    "\n",
    "        tokens = [token for line in tokens for token in line]\n",
    "    return collections.Counter(tokens)\n",
    "\n",
    "def truncate_pad(line, num_steps, padding_token):\n",
    "    if len(line) > num_steps:\n",
    "        return line[:num_steps] \n",
    "    return line + [padding_token] * (num_steps - len(line)) \n",
    "\n",
    "def seed_tensorflow(seed=42):\n",
    "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.set_random_seed(seed)\n",
    "    \n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions\n",
    "    \n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "    \n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "def test_rmse(model,X_test,Y_test):\n",
    "    test_preds = model.predict(X_test)\n",
    "    mse = mean_squared_error(Y_test, test_preds)\n",
    "    rmse = sqrt(mse)\n",
    "    return rmse\n",
    "    \n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    y_true = cast(y_true,float32)\n",
    "    return kb.sqrt(kb.mean(kb.square(y_pred - y_true)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data =pd.read_csv(\"~/autodl-tmp/full_length_reads.csv\")\n",
    "X = data['sequence']\n",
    "Y = data['copy_number']\n",
    "X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size = 0.2,random_state = 42)\n",
    "train_arr = build_kmers(X_train, 6)\n",
    "test_arr = build_kmers(X_test, 6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocab(train_arr,min_freq=200)\n",
    "print(list(vocab.token_to_idx.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 1600\n",
    "maxlen = num_steps\n",
    "x_train = [vocab[l] for l in train_arr]\n",
    "x_train = [l + [vocab['<eos>']] for l in x_train]\n",
    "x_train = [truncate_pad(l, num_steps, vocab['<pad>']) for l in x_train]\n",
    "\n",
    "x_test = [vocab[l] for l in test_arr]\n",
    "x_test = [l + [vocab['<eos>']] for l in x_test]\n",
    "x_test = [truncate_pad(l, num_steps, vocab['<pad>']) for l in x_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen)\n",
    "x_val = keras.preprocessing.sequence.pad_sequences(x_test, maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(embed_dim,num_heads,ff_dim,vocab_size,d_model):\n",
    "    inputs = layers.Input(shape=(maxlen,))\n",
    "    embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "    m = embedding_layer(inputs)\n",
    "    transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "    m = transformer_block(m)\n",
    "    m = layers.GlobalAveragePooling1D()(m)\n",
    "    m = layers.Dense(64, activation=\"relu\")(m)\n",
    "    m = layers.Dense(32, activation=\"relu\")(m)\n",
    "    outputs = layers.Dense(1, activation=\"linear\")(m)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    learning_rate = CustomSchedule(d_model)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n",
    "                                         epsilon=1e-9,global_clipnorm=0.5)\n",
    "    model.compile(optimizer=optimizer, loss=root_mean_squared_error)\n",
    "    return model\n",
    "\n",
    "# temp_learning_rate_schedule = CustomSchedule(d_model)\n",
    "# plt.plot(temp_learning_rate_schedule(tf.range(40000, dtype=tf.float32)))\n",
    "# plt.ylabel(\"Learning Rate\")\n",
    "# plt.xlabel(\"Train Step\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiplicand = int(X.shape[0]*0.2)\n",
    "X_list = []\n",
    "Y_list = []\n",
    "for i in range(0,5,1):\n",
    "    X_list.append(X[i*multiplicand:(i+1)*multiplicand])\n",
    "    Y_list.append(Y[i*multiplicand:(i+1)*multiplicand])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 32\n",
    "num_heads = 2 # Number of Attention Heads\n",
    "ff_dim = 128  # Hidden layer size in FFN\n",
    "vocab_size = len(vocab)\n",
    "d_model = 128\n",
    "num_steps = 1600\n",
    "maxlen = num_steps\n",
    "epoch_num = 20\n",
    "\n",
    "rmse = []\n",
    "for i in range(0,5,1):\n",
    "    X_test = X_list[i]\n",
    "    Y_test = Y_list[i]\n",
    "    X_train = []\n",
    "    Y_train = []\n",
    "    for j in range(0,5,1):\n",
    "        if j != i:\n",
    "            X_train.append(X_list[j])\n",
    "            Y_train.append(Y_list[j])\n",
    "    X_train = pd.concat(X_train,axis = 0)\n",
    "    Y_train = pd.concat(Y_train,axis = 0)\n",
    "    X_train = X_train.values.reshape(X_train.shape[0], )\n",
    "    X_test = X_test.values.reshape(X_test.shape[0], )\n",
    "    train_arr = build_kmers(X_train, 6)\n",
    "    test_arr = build_kmers(X_test, 6)\n",
    "    vocab = Vocab(train_arr,min_freq=200)\n",
    "    x_train = [vocab[l] for l in train_arr]\n",
    "    x_train = [l + [vocab['<eos>']] for l in x_train]\n",
    "    x_train = [truncate_pad(l, num_steps, vocab['<pad>']) for l in x_train]\n",
    "    x_test = [vocab[l] for l in test_arr]\n",
    "    x_test = [l + [vocab['<eos>']] for l in x_test]\n",
    "    x_test = [truncate_pad(l, num_steps, vocab['<pad>']) for l in x_test]\n",
    "    x_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen)\n",
    "    x_val = keras.preprocessing.sequence.pad_sequences(x_test, maxlen)\n",
    "    model = create_model(embed_dim,num_heads,ff_dim,vocab_size,d_model)\n",
    "    model.fit(x_train, Y_train, batch_size=64, epochs=epoch_num, validation_data=(x_val, Y_test), verbose=0)\n",
    "    rmse.append(test_rmse(model,x_val,Y_test))\n",
    "    print(rmse[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(rmse,columns=[\"Transformer\"]).to_csv(\"transformer_full_length.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
